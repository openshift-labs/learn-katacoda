# Circuit Breaking/Timeouts

Istio provides opt-in failure recovery and fault injection features that you
can configure dynamically at runtime. Using these features helps your
applications operate reliably, ensuring that the service mesh can tolerate
failing nodes and preventing localized failures from cascading to other
nodes.

## What we will learn in this module
This module will provide instruction on request retries, handling of service
timeouts, and how to apply circuit breaking. This module will also show how to
visualize these capabilities in Kiali.

## Before Starting
You only need the `customer` Virtual Service and Gateway, but if you have the
`recommendation` Destination Rule from other exercises, that's OK:

[source,bash,role#"execute-1"]
----
oc -n %username%-tutorial get istio-io
----

And you should see something like the following:

----
NAME                                           AGE
gateway.networking.istio.io/customer-gateway   3h16m

NAME                                          GATEWAYS             HOSTS   AGE
virtualservice.networking.istio.io/customer   [customer-gateway]   [*]     3h16m

NAME                                                 HOST             AGE
destinationrule.networking.istio.io/recommendation   recommendation   36m
----

If you have any scripts running in the bottom terminal, make sure to click
there and then press ctrl+c to terminate them.

[#retry]
## Retry

To add resilience to your application, you can ask the Service Mesh to retry
failed connections N more times. Istio by default implements retries for
`VirtualService` endpoints. You will make now make recommendation-v2 fail
100% of the time so that we can observe this retry behavior:

[source,bash,role#"execute-1"]
----
bash /opt/app-root/workshop/content/scripts/retries.sh misbehave
----

You will see something like:

----
Following requests to / will return a 503
----

This is a special endpoint that will toggle the recommendation application to
return only `503`s. You can validate that the v2 endpoint now only returns
`503` with the following:

[source,bash,role#"execute-1"]
----
bash /opt/app-root/workshop/content/scripts/curl_recommendation_args.sh v2
----

However, you will see that a normal request that goes through the entire set
of microservices will work every time. This is because the mesh is
implementing retries:

[source,bash,role#"execute-1"]
----
export INGRESS_GATEWAY#$(oc get route -n %username%-smcp istio-ingressgateway -o 'jsonpath#{.spec.host}')
for x in $(seq 1 10); do curl http://${INGRESS_GATEWAY}; done
----

Start generating traffic in the background:

[source,bash,role#"execute-2"]
----
bash /opt/app-root/workshop/content/scripts/curl_customer_quiet.sh
----

### Kiali's Graph
In the Kiali graph, you will notice 0% of the traffic going to v2. You can
click on the red V2 box and see no traffic is being logged:

[#img-503]
.Kiali Graph Retry
image:images/retry.png[]

Even though the recommendation v2 has no traffic logged, the client has a
100% success rate due to the retries.

Now, make the pod v2 behave well again:

[source,bash,role#"execute-1"]
----
bash /opt/app-root/workshop/content/scripts/retries.sh behave
----

You will see something like:

----
Following requests to / will return 200
----

The application is back to random load-balancing between v1, v2 and v3.

[#timeout]
## Timeout

Now we will configure a service to wait only N seconds before giving up and
failing.

First, introduce some wait time in `recommendation v2` by making it a slow
performer with a 3 second delay by running the command

[source,bash,role#"execute-1"]
----
bash /opt/app-root/workshop/content/scripts/retries.sh timeout?timeout#3
----

You will see something like:

----
Timeout has been set to 3 seconds
----

Keep the load script running.

In Kiali Graph, switch to viewing Response time (it's the second dropdown
menu, to the right of _Versioned app graph_).

See the load-balancing between v1, v2 and v3 but with v2 taking a bit of time
to respond

[#img-]
.Kiali Graph Showing slow v2 endpoint response time
image:images/circuit-breaking-graph-slowv2.png[]

If you click the "Distributed Tracing" button in Kiali, you can look at the
Jaeger traces for these calls. You'll see that the 3s time for the calls that
hit V2 make the trace look bizarre, because the rest of the calls take
single-digit _milliseconds_ in comparison.

*Note:* If you see an error when trying to access the distributed tracing
information, make sure that you visit the Jaeger UI directly in order to
accept the SSL certificate and grant permissions to Jaeger. In this lab
guide, you can do this by clicking on "Console" to open the OpenShift web
console, then click "Networking" -> "Routes" and make sure to select the
`%username%-smcp` project (at the top) in order to find the URL for Jaeger.

[#img-timeout-v1]
.Kiali Distributed Tracing for Recommendation v1
image:images/timeout-v1.png[]

[#img-timeout-v2]
.Kiali Distributed Tracing for Recommendation v2
image:images/timeout-v2.png[]

Note the duration of v2 is > 3s compared to the ms time of v1 and v3

Now, add the timeout rule:

[source,bash,role#"execute-1"]
----
oc create -n %username%-tutorial -f /opt/app-root/workshop/content/src/istiofiles/virtual-service-recommendation-timeout.yml
----

This file looks like:

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: recommendation
spec:
  hosts:
  - recommendation
  http:
  - route:
    - destination:
        host: recommendation
    timeout: 1.000s
----

This tells Istio to wait no longer than 1.000s before declaring the endpoint
timed out and retrying.

You will see it return v1 or v3 after waiting about 1 second. You don't see
v2 anymore because the response from v2 expires after the timeout period and
it is never returned:

[source,bash,role#"execute-1"]
----
export INGRESS_GATEWAY#$(oc get route -n %username%-smcp istio-ingressgateway -o 'jsonpath#{.spec.host}')
for x in $(seq 1 10); do curl http://${INGRESS_GATEWAY}; done
----

You can also observe this in Kiali:

[#img-timeout]
.Kiali Graph for Timeout Rule
image:images/timeout.png[]

Note that recommendation v2 now has a 100% failure rate due to the timeout
rule. Also note that the response time will never be less than 1s because
every third request, which would hit v2, ends up being timed out and retried.

### Clean up

Change the implementation of `v2` back to the image that responds without the
delay of 3 seconds:

[source,bash,role#"execute-1"]
----
bash /opt/app-root/workshop/content/scripts/retries.sh timeout?timeout#0
----

You will see something like:

----
Timeout has been set to 0 seconds
----

Then delete the virtual service created for timeout by:

[source,bash,role#"execute-1"]
----
oc delete -n %username%-tutorial -f /opt/app-root/workshop/content/src/istiofiles/virtual-service-recommendation-timeout.yml
----

You will see something like:

----
virtualservice.networking.istio.io "recommendation" deleted
----

Lastly, terminate the load script running in the bottom terminal with Control+C.

[#failfast]
## Fail Fast with Max Connections and Max Pending Requests

Let's use a 34/33/33 split of traffic:

[source,bash,role#"execute-1"]
----
oc create -n %username%-tutorial -f /opt/app-root/workshop/content/src/istiofiles/virtual-service-recommendation-split.yml
----

This YAML creates a `DestinationRule` and a `VirtualService` to control the traffic:

[source,yaml,subs#"+macros,+attributes"]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: recommendation
spec:
  hosts:
  - recommendation
  http:
  - route:
    - destination:
        host: recommendation
        subset: v1
      weight: 34
    - destination:
        host: recommendation
        subset: v2
      weight: 33
    - destination:
        host: recommendation
        subset: v3
      weight: 33
---
----

Note the weighting of the 3 recommendation destination versions.

Run the following a few times to generate some load for Kiali to capture:

[source,bash,role#"execute-1"]
----
export INGRESS_GATEWAY#$(oc get route -n %username%-smcp istio-ingressgateway -o 'jsonpath#{.spec.host}')
for x in $(seq 1 10); do curl http://${INGRESS_GATEWAY}; done
----

Now go to "Distributed Tracing" in Kiali and look at the response times. Note that all recommendation hits respond within single-digit milliseconds.

[#img-failfast]
.Kiali Distributed Tracing for Base Fail Fast
image:images/failfast.png[]

[#nocircuitbreaker]
### Load test without circuit breaker

Next, introduce some wait time in `recommendation v2` by making it a slow
performer with a 3 second delay by running the command:

[source,bash,role#"execute-1"]
----
bash /opt/app-root/workshop/content/scripts/retries.sh timeout?timeout#3
----

You will see something like:

----
Timeout has been set to 3 seconds
----

Run the following a few times to generate new, "delayed" traffic:

[source,bash,role#"execute-1"]
----
export INGRESS_GATEWAY#$(oc get route -n %username%-smcp istio-ingressgateway -o 'jsonpath#{.spec.host}')
for x in $(seq 1 10); do curl http://${INGRESS_GATEWAY}; done
----

[#img-nocicuit]
.Kiali Distributed Tracing for Fail Fast w/no Circuit Breaking
image:images/nocircuit.png[]

All of the requests to our system were successful, but 1/3 of the requests
took longer time, as the `v2` instance/pod was a slow performer.

[#circuitbreaker]
### Load test with circuit breaker

But suppose that in a production system this 3s delay was caused by too many
concurrent requests to the same instance/pod. We don't want multiple requests
getting queued or making the instance/pod even slower. So we'll add a circuit
breaker that will *open* whenever we have more than 1 request being handled
by any instance/pod:

[source,bash,role#"execute-1"]
----
oc apply -n %username%-tutorial -f /opt/app-root/workshop/content/src/istiofiles/destination-rule-recommendation_cb_policy_version_v2.yml
----

This file defines the Istio circuit breaker logic to prevent connections to
V2 from piling up:

[source,yaml,subs#"+macros,+attributes"]
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: recommendation
spec:
  host: recommendation
  subsets:
    - name: v1
      labels:
        version: v1
    - name: v2
      labels:
        version: v2
      trafficPolicy:
        connectionPool:
          http:
            http1MaxPendingRequests: 1
            maxRequestsPerConnection: 1
          tcp:
            maxConnections: 1
        outlierDetection:
          baseEjectionTime: 120.000s
          consecutiveErrors: 1
          interval: 1.000s
          maxEjectionPercent: 100
    - name: v3
      labels:
        version: v3
----

Note the connection pool with a max of 1 pending request and a traffic policy
where 100% of single consecutive errors fail. `oc apply` causes the existing
object(`DestinationRule` called `recommendation`) to be updated.

You will see something like:

----
destinationrule.networking.istio.io/recommendation configured
----

Now let's see what is the behavior of the system running some load again.
This script runs roughly 20 concurrent requests (it performs `curl` in the
background without waiting for the command to finish):

[source,bash,role#"execute-2"]
----
bash /opt/app-root/workshop/content/scripts/loadtest_quiet.sh
----

If you then look at the Kiali graph:

[#img-cicuit] []
.Kiali Graph Fail Fast w/Circuit Breaking
image:images/circuit-graph.png[]

When looking at request percentage, you will see a teeny tiny percentage of
requests hitting v2. That's the circuit breaker being opened whenever Istio
detects more than 1 pending request being handled by the v2 instance/pod. You
can also see the little lightning icon indicating a breaker is configured.

### Clean up

Change the implementation of `v2` back to the image that responds without the
delay of 3 seconds:

[source,bash,role#"execute-1"]
----
bash /opt/app-root/workshop/content/scripts/retries.sh timeout?timeout#0
----

You will see something like:

----
Timeout has been set to 0 seconds
----

Then delete the virtual service and the destination rule created for circuit braking by:

[source,bash,role#"execute-1"]
----
oc delete -n %username%-tutorial virtualservice.networking.istio.io/recommendation
----
